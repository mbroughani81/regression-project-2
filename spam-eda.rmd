```{r setup}
knitr::opts_chunk$set(cache = TRUE)
```

```{r}
load_libraries <- function() {
  library(readr)
  library(dplyr)
  library(tidyr)
  library(tibble)
  library(ggplot2)
  library(jalcal)
  library(purrr)
  library(lubridate)
  library(caret)
  library(pROC)
}
load_libraries()
```

```{r}
# products <- read_csv("data/digikala-products.csv")
# comments <- read_csv("data/digikala-comments.csv")
load("/media/mbroughani81/500A6DE90A6DCC92/uni/8/Regression/Project-2/data/products.rda")
load("/media/mbroughani81/500A6DE90A6DCC92/uni/8/Regression/Project-2/data/comments.rda")
```

Working with last 1 mil row to have better performance

```{r}
model_param_body_length <- 20
model_param_cnt_threshold <- 3
```
```{r}
# sampling
# cleaning
# adding the position in comment list (order by date)
sampled_comments <- comments %>%
  filter(nchar(body) > model_param_body_length) %>%
  filter(year(created_at_greg) == 2022 & month(created_at_greg) >= 11) %>%
  mutate(
    title = replace(title, title == "nan", ""),
    title = replace(title, is.na(title), ""),
    body = replace(body, body == "nan", "")
  ) %>%
  group_by(product_id) %>%
  mutate(row_in_asc_product_comments = min_rank(created_at_greg)) %>%
  mutate(row_in_desc_product_comments = min_rank(desc(created_at_greg))) %>%
  ungroup() %>%
  group_by(created_at_greg, body) %>%
  mutate(body_cnt_in_day = n()) %>%
  ungroup()
u_products <- products %>%
  distinct(id, .keep_all = TRUE)
sampled_comments <-
  left_join(sampled_comments, u_products, by = join_by(product_id == id))
```

```{r}
comments_cnt <- sampled_comments %>%
  group_by(body) %>%
  summarise(
    cnt = n()
  ) %>%
  ungroup() %>%
  arrange(desc(cnt))
```

# Logistic Regression
1. using duplicate comments as spams 
```{r}
duplicate_comments <- comments_cnt %>%
  filter(cnt >= model_param_cnt_threshold) %>%
  select(cnt, body)
duplicate_comments_list <- duplicate_comments[["body"]]
# labeling comments with duplicate body as spam
labeled_comments_sampled <- sampled_comments %>%
  mutate(spam = map_lgl(body, ~ any(.x %in% duplicate_comments_list)))
```

```{r, eval=FALSE}
# Checking if labelling is correct
View(duplicate_comments)
labeled_comments_sampled %>%
  filter(body == duplicate_comments_list[[1]]) %>%
  View()
```

2. creating the model
```{r}
comments_2 <- labeled_comments_sampled %>%
  mutate(
    F1_likes = likes,
    F2_dislikes = dislikes,
    F3_likes_dislikes_ratio = likes / (dislikes + 1),
    F4_title_length = nchar(title),
    F5_body_length = nchar(body),
    F6_asc_order = row_in_asc_product_comments,
    F7_desc_order = row_in_desc_product_comments,
    F8_product_price = price,
    F9_product_rating = rate.y,
    F10_body_cnt_in_day = body_cnt_in_day
  )
lg_model <- glm(
  spam ~ F1_likes + F2_dislikes + F3_likes_dislikes_ratio + F4_title_length +
    F5_body_length + F6_asc_order + F7_desc_order + F8_product_price +
    F9_product_rating + F10_body_cnt_in_day,
  data = comments_2,
  family = binomial
)
test_prob <- predict(lg_model, type = "response")
test_roc <- roc(comments_2$spam ~ test_prob, plot = TRUE, print.auc = TRUE)

get_logistic_pred <- function(mod, pos = "Yes", neg = "No", cut = 0.5) {
  probs <- predict(mod, type = "response")
  ifelse(probs > cut, pos, neg)
}
test_pred_10 <- get_logistic_pred(lg_model, cut = 0.1)
test_pred_50 <- get_logistic_pred(lg_model, cut = 0.5)
test_pred_90 <- get_logistic_pred(lg_model, cut = 0.9)
spam_chr <- ifelse(comments_2$spam, "Yes", "No")
test_tab_10 <- table(predicted = test_pred_10, actual = spam_chr)
test_tab_50 <- table(predicted = test_pred_50, actual = spam_chr)
test_tab_90 <- table(predicted = test_pred_90, actual = spam_chr)
# Will not work if there are "True"/"False" predictions
test_con_mat_10 <- confusionMatrix(test_tab_10, positive = "Yes")
test_con_mat_50 <- confusionMatrix(test_tab_50, positive = "Yes")
test_con_mat_90 <- confusionMatrix(test_tab_90, positive = "Yes")
# some important numbers
comments_2 %>%
  nrow()
actual_true <- comments_2 %>%
  filter(spam == TRUE) %>%
  nrow()
actual_false <- comments_2 %>%
  filter(spam == FALSE) %>%
  nrow()
actual_true
actual_false
# finding the threshold
lg_probs <- predict(lg_model, type = "response", na.action = stop)
length(lg_probs) == nrow(comments_2)
lg_pred <- rep(FALSE, length(lg_probs))
lg_pred[lg_probs > 0.15] <- TRUE
# creating new_comments_2, comments_2 with the prediction
new_comments_2 <- add_column(.data = comments_2, prediction = lg_pred)
TP <- new_comments_2 %>%
  filter(spam == TRUE & prediction == TRUE) %>%
  nrow()
TN <- new_comments_2 %>%
  filter(spam == FALSE & prediction == FALSE) %>%
  nrow()
FP <- new_comments_2 %>%
  filter(spam == FALSE & prediction == TRUE) %>%
  nrow()
FN <- new_comments_2 %>%
  filter(spam == TRUE & prediction == FALSE) %>%
  nrow()
TN / (TN + FP)
TP / (TP + FN)
```

# More analysis

Checking the FP and FN, and also the spams, 

```{r}

```
