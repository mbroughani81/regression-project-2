```{r setup}
knitr::opts_chunk$set(cache = TRUE)
```

```{r}
load_libraries <- function() {
  library(readr)
  library(dplyr)
  library(tidyr)
  library(tibble)
  library(ggplot2)
  library(jalcal)
  library(purrr)
  library(lubridate)
  library(caret)
  library(pROC)
  library(stringr)
}
load_libraries()
```

```{r}
# products <- read_csv("data/digikala-products.csv")
# comments <- read_csv("data/digikala-comments.csv")
# load("/media/mbroughani81/500A6DE90A6DCC92/uni/8/Regression/Project-2/data/products.rda")
# load("/media/mbroughani81/500A6DE90A6DCC92/uni/8/Regression/Project-2/data/comments.rda")
load("/mnt/uni/8/Regression/Project-2/data/products.rda")
load("/mnt/uni/8/Regression/Project-2/data/comments.rda")

comments <- comments %>%
  distinct(id, .keep_all = TRUE)
u_products <- products %>%
  distinct(id, .keep_all = TRUE)
comments <- comments %>%
  group_by(product_id) %>%
  mutate(row_in_asc_product_comments = min_rank(created_at_greg)) %>%
  mutate(row_in_desc_product_comments = min_rank(desc(created_at_greg))) %>%
  ungroup() %>%
  mutate(
    title = replace(title, title == "nan", ""),
    title = replace(title, is.na(title), ""),
    body = replace(body, body == "nan", "")
  )
```

Working with last 1 mil row to have better performance

```{r}
model_param_body_length <- 20
model_param_cnt_threshold <- 3
```
```{r}
# sampling
# cleaning
# adding the position in comment list (order by date)
comments_1 <- comments %>%
  filter(year(created_at_greg) == 2022 & month(created_at_greg) >= 11)
sampled_comments <- comments_1 %>%
  filter(nchar(body) > model_param_body_length) %>%
  group_by(product_id) %>%
  mutate(row_in_asc_product_comments = min_rank(created_at_greg)) %>%
  mutate(row_in_desc_product_comments = min_rank(desc(created_at_greg))) %>%
  ungroup() %>%
  group_by(created_at_greg, body) %>%
  mutate(body_cnt_in_day = n()) %>%
  ungroup()
sampled_comments <-
  left_join(sampled_comments, u_products, by = join_by(product_id == id))
comments_cnt <- sampled_comments %>%
  group_by(body) %>%
  summarise(
    cnt = n()
  ) %>%
  ungroup() %>%
  arrange(desc(cnt))
```

# Logistic Regression
1. using duplicate comments as spams 
```{r}
duplicate_comments <- comments_cnt %>%
  filter(cnt >= model_param_cnt_threshold) %>%
  select(cnt, body)
duplicate_comments_list <- duplicate_comments[["body"]]
# labeling comments with duplicate body as spam
labeled_sampled_comments <- sampled_comments %>%
  mutate(spam = map_lgl(body, ~ any(.x %in% duplicate_comments_list)))
```

```{r, eval=FALSE}
# Checking if labelling is correct
View(duplicate_comments)
labeled_comments_sampled %>%
  filter(body == duplicate_comments_list[[1]]) %>%
  View()
# Checking the number of spams per product.
# If a product has a lot of spams, smth is wrong!
labeled_sampled_comments %>%
  group_by(product_id) %>%
  summarise(spam_cnt = sum(spam)) %>%
  ungroup() %>%
  arrange(desc(spam_cnt)) %>%
  View()
labeled_sampled_comments %>%
  filter(product_id == 7734200) %>%
  select(spam, is_buyer, created_at_greg, body) %>%
  View()
labeled_sampled_comments %>%
  filter(product_id == 9644579) %>%
  select(spam, created_at_greg, body) %>%
  View()
```

2. creating the model
```{r}
# creatingn model
comments_2 <- labeled_sampled_comments %>%
  mutate(
    F1_likes = likes,
    F2_dislikes = dislikes,
    F3_likes_dislikes_ratio = likes / (dislikes + 1),
    F4_title_length = nchar(title),
    F5_body_length = nchar(body),
    F6_asc_order = row_in_asc_product_comments,
    F7_desc_order = row_in_desc_product_comments,
    F8_product_price = price,
    F9_product_rating = rate.y,
    F10_body_cnt_in_day = body_cnt_in_day
  )
lg_model <- glm(
  spam ~ F1_likes + F2_dislikes + F3_likes_dislikes_ratio + F4_title_length +
    F5_body_length + F6_asc_order + F7_desc_order + F8_product_price +
    F9_product_rating + F10_body_cnt_in_day,
  data = comments_2,
  family = binomial
)
# roc
test_prob <- predict(lg_model, type = "response")
test_roc <- roc(comments_2$spam ~ test_prob, plot = TRUE, print.auc = TRUE)
# confusion matrix
get_logistic_pred <- function(mod, pos = "Yes", neg = "No", cut = 0.5) {
  probs <- predict(mod, type = "response")
  ifelse(probs > cut, pos, neg)
}
test_pred_10 <- get_logistic_pred(lg_model, cut = 0.1)
test_pred_50 <- get_logistic_pred(lg_model, cut = 0.5)
test_pred_90 <- get_logistic_pred(lg_model, cut = 0.9)
spam_chr <- ifelse(comments_2$spam, "Yes", "No")
test_tab_10 <- table(predicted = test_pred_10, actual = spam_chr)
test_tab_50 <- table(predicted = test_pred_50, actual = spam_chr)
test_tab_90 <- table(predicted = test_pred_90, actual = spam_chr)
# Will not work if there are "True"/"False" predictions
test_con_mat_10 <- confusionMatrix(test_tab_10, positive = "Yes")
test_con_mat_50 <- confusionMatrix(test_tab_50, positive = "Yes")
test_con_mat_90 <- confusionMatrix(test_tab_90, positive = "Yes")
# some important stats
comments_2 %>%
  nrow()
actual_true <- comments_2 %>%
  filter(spam == TRUE) %>%
  nrow()
actual_false <- comments_2 %>%
  filter(spam == FALSE) %>%
  nrow()
actual_true
actual_false
# finding the threshold
lg_probs <- predict(lg_model, type = "response", na.action = stop)
length(lg_probs) == nrow(comments_2)
lg_pred <- rep(FALSE, length(lg_probs))
lg_pred[lg_probs > 0.15] <- TRUE
new_comments_2 <- add_column(
  .data = comments_2,
  prediction = lg_pred
) # creating new_comments_2, comments_2 with the prediction
TP <- new_comments_2 %>%
  filter(spam == TRUE & prediction == TRUE) %>%
  nrow()
TN <- new_comments_2 %>%
  filter(spam == FALSE & prediction == FALSE) %>%
  nrow()
FP <- new_comments_2 %>%
  filter(spam == FALSE & prediction == TRUE) %>%
  nrow()
FN <- new_comments_2 %>%
  filter(spam == TRUE & prediction == FALSE) %>%
  nrow()
TN / (TN + FP)
TP / (TP + FN)
```

# More analysis

## Checking the FP and FN, and also the spams, 

Checking the spams, that are based on duplication
```{r, eval=FALSE}
# number of is_buyers
duplicate_comments %>%
  head(100) %>%
  View()
```

Timeseries of pos/neg comments on a product

```{r, eval=FALSE}
comments %>%
  head(50) %>%
  select(rate, body) %>%
  View()
arrange(desc(day_count))

product_comments_ts %>%
  head(100) %>%
  View()

comments$rate %>%
  unique() %>%
  sort() %>%
  View()
comments %>%
  filter(rate == 2500) %>%
  head(10) %>%
  View()
comments %>%
  filter(is_buyer == FALSE) %>%
  head(20) %>%
  View()

# Filtering the comments with zero rate, to see the anomalies better
comments_3 <- comments %>%
  filter(rate != 0)

product_comments_ts <- comments_3 %>% # comments %>%
  arrange(created_at_greg) %>%
  mutate(
    recom_status = case_when(
      rate >= 0 & rate < 2 ~ "Bad",
      rate >= 2 & rate < 4 ~ "Normal",
      rate >= 4 ~ "Good"
    )
  ) %>%
  group_by(product_id) %>%
  mutate(
    day_count = length(unique(created_at_greg))
  ) %>%
  ungroup() %>%
  arrange(desc(day_count))

p_ids <- (product_comments_ts %>% select(product_id))[["product_id"]] %>%
  unique()
product_comments_ts %>%
  filter(product_id %in% p_ids[1:5]) %>%
  ggplot() +
  geom_point(aes(x = created_at_greg, y = cnt, color = recom_status)) +
  facet_grid(product_id ~ .)
product_comments_ts %>%
  filter(product_id %in% p_ids[55:60]) %>% ## 55-60
  ggplot() +
  geom_histogram(aes(x = created_at_greg, color = recom_status), binwidth = 30) +
  facet_grid(recom_status ~ product_id)
# sudden only pos/neg change
# just one month increase in comment
## 956949 2437016, 506914, 770216 , 2145003, 1167347, 2448826
product_comments_ts %>%
  filter(product_id %in% c(956949, 770216, 82078)) %>% ## 55-60
  ggplot() +
  geom_histogram(aes(x = created_at_greg, color = recom_status), binwidth = 30) +
  facet_grid(recom_status ~ product_id)
product_comments_ts %>%
  filter(product_id == 956949) %>%
  select(created_at_greg, rate, body) %>%
  View()
```
```{r}
# 1. sudden Bad Rates
product_comments_ts %>%
  filter(product_id %in% c(
    629761, 76937, 82138, 153035, 170694, 298695,
    37424, 185959
  )) %>%
  ggplot() +
  geom_point(aes(x = created_at_greg, y = cnt)) +
  facet_grid(recom_status ~ product_id)
# What is the reason for sudden bad rates?
# 37424, 185959: between 2021-10 to 2022-6, all comments have rate == 0
product_comments_ts %>%
  filter(product_id == 185959) %>%
  ggplot() +
  geom_point(aes(x = created_at_greg, y = cnt)) +
  facet_grid(recom_status ~ product_id)
comments %>%
  filter(product_id == 185959) %>%
  select(created_at_greg, rate, is_buyer, body) %>%
  arrange(created_at_greg) %>%
  View()

# 2. after a time (start of 2022), the number of ratings with bad rating generally decresses
#    reason: after 2022, number of zero rates become much less!
product_comments_ts %>%
  filter(product_id %in% c(799907, 684093, 317249)) %>%
  ggplot() +
  geom_point(aes(x = created_at_greg, y = cnt, color = recom_status)) +
  facet_grid(product_id ~ .)
product_comments_ts %>%
  filter(product_id == 684093) %>%
  ggplot() +
  geom_point(aes(x = created_at_greg, y = cnt)) +
  facet_grid(recom_status ~ product_id)
comments %>%
  filter(product_id == 684093) %>%
  select(created_at_greg, rate, is_buyer, body) %>%
  arrange(created_at_greg) %>%
  View()
# 317249: after a time, less rates no zero rates are placed
# zero rates???
comments %>%
  filter(rate == 0) %>%
  count(created_at_greg) %>%
  ggplot() +
  geom_point(aes(x = created_at_greg, y = n))
```

```{r}
# Fixing comments. No zero rate??
comments %>%
  group_by(created_at_greg) %>%
  summarise(
    cnt_zero = sum(rate == 0),
    cnt_all = length(rate),
    q = cnt_zero / cnt_all
  ) %>%
  ungroup() %>%
  ggplot() +
  geom_point(aes(x = created_at_greg, y = q))
```

Labeling a sample of comments.
Type 1: non-related
Type 2: brand related 

```{r}
set.seed(1)
sample_count <- 10000
labeling_sample <- comments %>%
  sample_n(sample_count, replace = FALSE)

non_product_words_mention_percent <- function(body, seller_title, brand) {
  commons <- c(
    "دیجیکالا",
    "دیجی کالا",
    "ارسال",
    "بسته بندی",
    "برند",
    "مارک",
    "تصویر",
    "فروشنده",
    "موجود",
    "اشتباه",
    "اشتباهی",
    "انقضا"
  )
  sum_counts <- 0
  for (str in commons) {
    sum_counts <- sum_counts + str_count(body, str)
  }
  # print(sum_counts)
  fff <- function(x1, x2) {
    return(str_count(x1, x2))
  }
  sum_counts <- sum_counts + mapply(fff, body, seller_title)
  sum_counts <- sum_counts + mapply(fff, body, brand)
  word_count <- str_count(body, "\\w+")
  # print(sum_counts)
  # print(word_count)
  return(sum_counts / (word_count + 1))
}

bb1 <- " خیلی وقته منتظر سایز۴۶م ولی موجود نمیشه. لطفا موجودکنید عزیزان دیانی و دیجی کالا"
bb2 <- " خیلی وقته منتظر سایز۴۶م ولی موجود نمیشه. لطفا موجودکنید عزیزان  و دیجی کالا"
bb3 <- ""

cc <- c("a", "b", "c")
str_count(bb, "موجود")
non_product_words_mention_percent(c(bb1, bb2, bb3), cc)
non_product_words_mention_percent(c("a b c d", "b b", "c d"), c("a", "b", "c"))
cnt
```

Manual Labeling

```{r}
labeling_sample$type_1_non_related <- FALSE
labeling_sample$type_2_brand_related <- FALSE
labeling_sample <- left_join(
  labeling_sample,
  u_products,
  by = join_by(product_id == id)
)
change_id_type_1 <- function(labeling_sample, ids, cnt) {
  cur_id <- ids[cnt]
  labeling_sample <- labeling_sample %>%
    mutate(type_2_brand_related = ifelse(
      id == cur_id,
      TRUE,
      type_2_brand_related
    ))
  x <- select(labeling_sample, id, type_1_non_related, type_2_brand_related)
  save(
    x,
    file = paste("type_1_", cnt, ".rda", sep = "")
  )
  return(labeling_sample)
}

ids <- labeling_sample$id
cnt
{
  cnt <- cnt + 1
  labeling_sample %>%
    filter(id == ids[cnt]) %>%
    select(body, title_fa, is_buyer, type_1_non_related, type_2_brand_related) %>%
    View()
}

labeling_sample <- change_id_type_1(labeling_sample, ids, cnt)
```


## Model for detecting type 1 and type 2

1. Using all the dataset
```{r}
load("labels_2.rda")
type_2_labels$type_2_brand_related %>% sum()
comments_labeled <- left_join(
  comments,
  type_2_labels,
  by = join_by(id == id)
)
comments_labeled <- comments_labeled %>%
  mutate(type_2_brand_related = replace_na(type_2_brand_related, FALSE))
comments_labeled <- left_join(
  comments_labeled,
  u_products,
  by = join_by(product_id == id)
)
comments_labeled %>% colnames()
comments_labeled %>% nrow()
```

```{r}
# creatingn model
comments_4 <- comments_labeled %>%
  mutate(
    F1_likes = likes,
    F2_dislikes = dislikes,
    F3_likes_dislikes_ratio = likes / (dislikes + 1),
    F4_title_length = nchar(title),
    F5_body_length = nchar(body),
    F6_asc_order = row_in_asc_product_comments,
    F7_desc_order = row_in_desc_product_comments,
    F8_product_price = price,
    F9_product_rating = rate.y,
  )
lg_model <- glm(
  type_2_brand_related ~ F1_likes + F2_dislikes + F3_likes_dislikes_ratio +
    F4_title_length + F5_body_length + F6_asc_order + F7_desc_order +
    F8_product_price + F9_product_rating,
  data = comments_4,
  family = binomial
)
# roc
nrow(comments_4)
test_prob <- predict(lg_model, type = "response")
length(test_prob)
nrow(comments_4)
test_roc <- roc(comments_4$type_2_brand_related ~ test_prob, plot = TRUE, print.auc = TRUE)
```

2. Using sampled dataset
```{r}
load("labels_2.rda")
type_2_labels$type_2_brand_related %>% sum()
comments_labeled <- left_join(
  type_2_labels,
  labeling_sample,
  by = join_by(id == id)
)
comments_labeled <- left_join(
  comments_labeled,
  u_products,
  by = join_by(product_id == id)
)
comments_labeled %>% colnames()
```

```{r}
# creating model

comments_labeled %>%
  colnames() %>%
  View()
comments_labeled %>%
  head(5) %>%
  View()
comments %>%
  head(5) %>%
  View()
u_products %>%
  head(5) %>%
  View()

comments_labeled %>%
  filter(type_2_brand_related == TRUE) %>%
  View()
comments_4 <- comments_labeled %>%
  mutate(
    F1_likes = likes,
    F2_dislikes = dislikes,
    F3_likes_dislikes_ratio = likes / (dislikes + 1),
    F4_title_length = nchar(title),
    F5_body_length = nchar(body),
    F6_asc_order = row_in_asc_product_comments,
    F7_desc_order = row_in_desc_product_comments,
    F8_product_price = price,
    F9_product_rating = rate.y,
    F10_non_product_words_mention_percent = non_product_words_mention_percent(
      body,
      seller_title,
      brand
    )
    # F11_cosine_sim_with_product_desc = 0
  )
comments_4 %>%
  select(F10_non_product_words_mention_percent) %>%
  head(10) %>%
  View()
comments_4 %>%
  select(type_2_brand_related) %>%
  distinct()
lg_model <- glm(
  type_2_brand_related ~ F1_likes + F2_dislikes + F3_likes_dislikes_ratio +
    F4_title_length + F5_body_length + F6_asc_order + F7_desc_order +
    F8_product_price + F9_product_rating +
    F10_non_product_words_mention_percent,
  data = comments_4,
  family = binomial
)
# roc
test_prob <- predict(lg_model, type = "response")
test_roc <- roc(comments_4$type_2_brand_related ~ test_prob, plot = TRUE, print.auc = TRUE)
```
